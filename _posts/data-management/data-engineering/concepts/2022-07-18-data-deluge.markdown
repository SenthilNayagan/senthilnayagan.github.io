---
layout: post
title:  "Data Deluge"
kicker: "Data Management"
subtitle: "When the granularity of data increases, its complexity also increases. At some point, we will reach a point where we cannot handle the volume of fresh data being generated."
image: assets/images/posts-cover-images/data-deluge.webp
author: senthil
date: 2022-07-18 20:04:24 +0530
tags: ["data-management", "data-engineering"]
categories: data-management
featured: false
hidden: false
---

# What is data deluge?

The word "deluge" is pronounced as *del-yooj* and it literally means an *enormous amount of anything*. 

A data deluge is a situation in which the amount of new data being produced is so enormous that organisations are unable to handle it. We have reached a point where the volume of data is expanding faster than the infrastructure and technologies that can support it. The granularity of the collected data intensifies the difficulty.

> **Data Granularity:** When data is split, it becomes more granular. It grew more specific and its complexity increased. A good example of data granularity is the subdivision of an address field into street, city, and zip code. Nevertheless, granularity grows when the unit is subdivided more.

When organisations are unable to handle the volume of new data being produced, it makes it impossible for analysts to analyse it and for researchers to draw any meaningful conclusions from it.

In addition to conventional data sources, there are many digital sources nowadays, such as social media. It is challenging to efficiently gather, visualise, and analyse information given the enormous volumes of data available.

## How can a data deluge be avoided?

It is essential to combat the data deluge by gathering the right amount of data that not only saves money but also offers significant and valuable insights.
