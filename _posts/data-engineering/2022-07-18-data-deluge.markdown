---
layout: post
title:  "Data Deluge"
date:   2022-07-18 20:04:24 +0530
categories: dataengineering
permalink: /:categories/:title
---
# What is data deluge?
The word "deluge" is pronounced as del-yooj and it literally means an enormous amount of anything. A data deluge is a situation in which the amount of new data being produced is so enormous that organisations are unable to handle it. We have reached a point where the volume of data is expanding faster than the infrastructure and technologies that can support it. The granularity of the collected data intensifies the difficulty.

> **Data Granularity:** When data is split, it becomes more granular. It grew more specific and its complexity increased. A good example of data granularity is the subdivision of an address field into street, city, and zip code. Nevertheless, granularity grows when the unit is subdivided more.

When organisations are unable to handle the volume of new data being produced, it makes it impossible for analysts to analyse it and for researchers to draw any meaningful conclusions from it.

In addition to conventional data sources, there are many digital sources nowadays, such as social media. It is challenging to efficiently gather, visualise, and analyse information given the enormous volumes of data available.

# How can a data deluge be avoided?
It is essential to combat the data deluge by gathering the right amount of data that not only saves money but also offers significant and valuable insights.
